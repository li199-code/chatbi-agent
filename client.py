"""
å¤šæœåŠ¡å™¨ MCP + LangChain Agent ç¤ºä¾‹
---------------------------------
1. è¯»å– .env ä¸­çš„ LLM_API_KEY / BASE_URL / MODEL
2. è¯»å– servers_config.json ä¸­çš„ MCP æœåŠ¡å™¨ä¿¡æ¯
3. å¯åŠ¨ MCP æœåŠ¡å™¨ï¼ˆæ”¯æŒå¤šä¸ªï¼‰
4. å°†æ‰€æœ‰å·¥å…·æ³¨å…¥ LangGraph Agentï¼Œç”±å¤§æ¨¡å‹è‡ªåŠ¨é€‰æ‹©å¹¶è°ƒç”¨
"""

import asyncio
import json
import logging
import os
from typing import Any, Dict, List

from dotenv import load_dotenv
from langgraph.prebuilt import create_react_agent
from langchain_deepseek import ChatDeepSeek
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import InMemorySaver

# è®¾ç½®è®°å¿†å­˜å‚¨
checkpointer = InMemorySaver()

# è¯»å–æç¤ºè¯
with open("agent_prompts.txt", "r", encoding="utf-8") as f:
    prompt = f.read()

# è®¾ç½®å¯¹è¯é…ç½®
config = {
    "configurable": {
        "thread_id": "1"  
    }
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç¯å¢ƒé…ç½®
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Configuration:
    """è¯»å– .env ä¸ servers_config.json"""

    def __init__(self) -> None:
        load_dotenv()
        self.api_key: str = os.getenv("LLM_API_KEY") or ""
        self.base_url: str | None = os.getenv("BASE_URL")  # DeepSeek ç”¨ https://api.deepseek.com
        self.model: str = os.getenv("MODEL") or "deepseek-chat"
        if not self.api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° LLM_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®")

    @staticmethod
    def load_servers(file_path: str = "servers_config.json") -> Dict[str, Any]:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f).get("mcpServers", {})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»é€»è¾‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run_chat_loop() -> None:
    """å¯åŠ¨ MCP-Agent èŠå¤©å¾ªç¯"""
    cfg = Configuration()
    os.environ["DEEPSEEK_API_KEY"] = os.getenv("LLM_API_KEY", "")
    if cfg.base_url:
        os.environ["DEEPSEEK_API_BASE"] = cfg.base_url
    servers_cfg = Configuration.load_servers()

    # 1ï¸ è¿æ¥å¤šå° MCP æœåŠ¡å™¨
    mcp_client = MultiServerMCPClient(servers_cfg)

    tools = await mcp_client.get_tools()         # LangChain Tool å¯¹è±¡åˆ—è¡¨

    logging.info(f"âœ… å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·ï¼š {[t.name for t in tools]}")

    # 2ï¸ åˆå§‹åŒ–å¤§æ¨¡å‹ï¼ˆDeepSeek / OpenAI / ä»»æ„å…¼å®¹ OpenAI åè®®çš„æ¨¡å‹ï¼‰
    DeepSeek_API_KEY = os.getenv("DEEPSEEK_API_KEY")
    model = ChatDeepSeek(model="deepseek-chat")

    # 3 æ„é€  LangGraph Agent
    
    agent = create_react_agent(model=model, 
                               tools=tools,
                               prompt=prompt,
                               checkpointer=checkpointer)

    # 4 CLI èŠå¤©
    print("\nğŸ¤– MCP Agent å·²å¯åŠ¨ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    while True:
        user_input = input("\nä½ : ").strip()
        if user_input.lower() == "quit":
            break
        try:
            result = await agent.ainvoke(
            {"messages": [{"role": "user", "content": user_input}]},
            config
        )
            print(f"\nAI: {result['messages'][-1].content}")
        except Exception as exc:
            print(f"\nâš ï¸  å‡ºé”™: {exc}")

    # 5ï¸ æ¸…ç†
    await mcp_client.cleanup()
    print("ğŸ§¹ èµ„æºå·²æ¸…ç†ï¼ŒBye!")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    asyncio.run(run_chat_loop()) 